{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Visit the Wikipedia hyperlinks graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group #2 Riccardo Cervelli, Davide Facchinelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WIKI](https://www.balcanicaucaso.org/var/obc/storage/images/articoli-da-pubblicare-2/turchia-oscurata-wikipedia/1221571-1-ita-IT/Turchia-oscurata-Wikipedia.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import statistics\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import igraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Goal of this assignment is to perform an analysis of the Wikipedia Hyperlink graph. In particular, given extra information about the categories to which an article belongs to, we ranked the articles according to some criteria that we'll explain you later.\n",
    "For this purpose we use the Wikipedia graph released by the SNAP group and you can check the files that we used [here](https://drive.google.com/file/d/1ghPJ4g6XMCUDFQ2JPqAVveLyytG8gBfL/view?usp=sharing), in order to compute find:\n",
    "- The number of nodes.\n",
    "- The number of edges.\n",
    "- The average node degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are building our graph reading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:32.836315Z",
     "start_time": "2018-12-14T20:52:29.303717Z"
    }
   },
   "outputs": [],
   "source": [
    "# we inizialize our graph as a default dictionary\n",
    "graph = defaultdict(list)\n",
    "\n",
    "# and populate it from our file\n",
    "with open('wiki-topcats-reduced.txt') as f:\n",
    "    for row in f.readlines():\n",
    "        edges = list(map(int,row.split('\\t')))\n",
    "        graph[edges[0]].append(edges[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are building the set of all the vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.347548Z",
     "start_time": "2018-12-14T20:52:32.957163Z"
    }
   },
   "outputs": [],
   "source": [
    "total_vertex = list(graph.keys())\n",
    "for k in graph.keys():\n",
    "    total_vertex+=graph[k]\n",
    "total_vertex = set(total_vertex)\n",
    "indicizer = sorted(list(total_vertex))\n",
    "\n",
    "# we prepare a dictonary that given a vertex it gives us it's index\n",
    "vertex_to_index = {indicizer[i]:i for i in range(len(indicizer))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the Number of vertex :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.351438Z",
     "start_time": "2018-12-14T20:52:34.348444Z"
    }
   },
   "outputs": [],
   "source": [
    "n_vert = len(total_vertex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate  Number of edges :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.794360Z",
     "start_time": "2018-12-14T20:52:34.352436Z"
    }
   },
   "outputs": [],
   "source": [
    "n_edges = sum(len(graph[k]) for k in total_vertex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the Density of our graph ($D$) :\n",
    "$D ={\\frac  {n_v}{n_v-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.798244Z",
     "start_time": "2018-12-14T20:52:34.795250Z"
    }
   },
   "outputs": [],
   "source": [
    "density = n_edges/(n_vert*(n_vert-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the Avreage node degree ($AND$) :  $AND ={\\frac  {2n_e}{n_v}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.816296Z",
     "start_time": "2018-12-14T20:52:34.799241Z"
    }
   },
   "outputs": [],
   "source": [
    "avreage_degree = 2*n_edges/n_vert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.835147Z",
     "start_time": "2018-12-14T20:52:34.818189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of vertex is :  461193\n",
      "The Number of edges is : 2645247\n",
      "The density of our graph is : 1.2436602635647606e-05\n",
      "The Average node Degree is : 11.471323285479182\n"
     ]
    }
   ],
   "source": [
    "print('The Number of vertex is : ',n_vert)\n",
    "print('The Number of edges is :',n_edges)\n",
    "print('The density of our graph is :',density)\n",
    "print('The Average node Degree is :',avreage_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First question : Is the graph dense?\n",
    "In mathematics, a dense graph is a graph in which the number of edges is close to the maximal number of edges. We calculated the density as $D={\\frac  {|E|}{|V|\\,(|V|-1)}}$ and we can see that is quite low. So no, the graph is not dense. We will show you later our plots, where yourselfe can observe the density of our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go to the next step.\n",
    "- Load the categories (take into account all the categories that have a number of articles greater than 3500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:36.696674Z",
     "start_time": "2018-12-14T20:52:35.437713Z"
    }
   },
   "outputs": [],
   "source": [
    "# we inizialize it as an empty dictonary\n",
    "category_dic = {}\n",
    "\n",
    "# and load the category from the file\n",
    "with open('wiki-topcats-categories.txt') as f:\n",
    "    for row in f.readlines():\n",
    "        category = row[:-2].split(' ')\n",
    "        elements = set(map(int,category[1:]))\n",
    "        # checking if the category is big enough\n",
    "        if len(elements)>3500:\n",
    "            # and taking only verteces that exist in our starting graph\n",
    "            category_dic[category[0][9:-1]] = elements & total_vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:36.701600Z",
     "start_time": "2018-12-14T20:52:36.697658Z"
    }
   },
   "outputs": [],
   "source": [
    "# we create a temporary second dictonary\n",
    "category_dic2 = {}\n",
    "# and we save in it only the category that are not empty\n",
    "for k in category_dic.keys():\n",
    "    if category_dic[k]:\n",
    "        category_dic2[k] = category_dic[k]\n",
    "category_dic = category_dic2\n",
    "del category_dic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:36.721586Z",
     "start_time": "2018-12-14T20:52:36.702597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can use only 35 categories.\n"
     ]
    }
   ],
   "source": [
    "print('We can use only',len(category_dic),'categories.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how big these categories are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:38.124791Z",
     "start_time": "2018-12-14T20:52:38.007106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English_footballers 7537\n",
      "The_Football_League_players 7813\n",
      "Association_football_forwards 5097\n",
      "Association_football_goalkeepers 3736\n",
      "Association_football_midfielders 5826\n",
      "Association_football_defenders 4589\n",
      "Living_people 348299\n",
      "Year_of_birth_unknown 2536\n",
      "Harvard_University_alumni 5548\n",
      "Major_League_Baseball_pitchers 5193\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies 6491\n",
      "Indian_films 5567\n",
      "Year_of_death_missing 4122\n",
      "English_cricketers 3275\n",
      "Year_of_birth_missing_(living_people) 28498\n",
      "Rivers_of_Romania 7728\n",
      "Main_Belt_asteroids 11659\n",
      "Asteroids_named_for_people 4895\n",
      "English-language_albums 4760\n",
      "English_television_actors 3361\n",
      "British_films 4422\n",
      "English-language_films 22462\n",
      "American_films 15158\n",
      "Fellows_of_the_Royal_Society 3446\n",
      "People_from_New_York_City 4615\n",
      "American_Jews 3411\n",
      "American_television_actors 11531\n",
      "American_film_actors 13865\n",
      "Debut_albums 7561\n",
      "Black-and-white_films 10758\n",
      "Year_of_birth_missing 4346\n",
      "Place_of_birth_missing_(living_people) 5532\n",
      "Article_Feedback_Pilot 3485\n",
      "American_military_personnel_of_World_War_II 3720\n",
      "Windows_games 4025\n"
     ]
    }
   ],
   "source": [
    "for item in category_dic.items():\n",
    "    print(item[0],len(item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a function from our file we get the list of categories ordered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define  a BFS function implemented from scratch ,finding all the shortest distance from a vertex to all the others.\n",
    "Notice that if there is not a way to go from a node to another we assigned as distance the value $-1$. Classically in the literature the value assigned in those cases is $\\infty$, and further in the code we will substitue the $-1$ value with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:41.832466Z",
     "start_time": "2018-12-14T20:52:41.827479Z"
    }
   },
   "outputs": [],
   "source": [
    "# calssic BFS algorithm\n",
    "\n",
    "def path_len(start,graph,indicizer):\n",
    "    \n",
    "    verteces = defaultdict(lambda:(False,np.infty))\n",
    "    verteces[start] = (True,0)\n",
    "    \n",
    "    queue = [start]\n",
    "\n",
    "    while queue:\n",
    "        actual = queue.pop(0)\n",
    "        d = verteces[actual][1]\n",
    "        for child in graph[actual]:\n",
    "            if not verteces[child][0]:\n",
    "                verteces[child] = (True,d+1)\n",
    "                queue.append(child)\n",
    "    result = []\n",
    "    for k in indicizer:\n",
    "        result.append(verteces[k][1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we input our input category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T11:49:21.879935Z",
     "start_time": "2018-12-14T11:49:20.816688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input category: Year_of_birth_unknown\n"
     ]
    }
   ],
   "source": [
    "input_category = input('Input category: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get all the distances from the vertex of the input category to all the other vertices. As the files are really big we save them on the drive because we tryed to don't overload the RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T11:32:31.446300Z",
     "start_time": "2018-12-14T08:29:00.217074Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2536/2536 [2:38:49<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# we apply the BFS algorithm to each element of the input_category\n",
    "vdist = dict()\n",
    "# we inzialize two counters\n",
    "# one to count how many start vertex we have stored\n",
    "vx = 0\n",
    "# the other will set the name of the file where we are going to save them\n",
    "files = 0\n",
    "for v in tqdm(category_dic[input_category]):\n",
    "    # we compure the distance from v to each other vertex\n",
    "    vdist[v] = path_len(v,graph,indicizer)\n",
    "    vx+=1\n",
    "    # and each 100 vertex we save it on a file\n",
    "    if vx == 100:\n",
    "        files+=1\n",
    "        with open('dist_dict_'+str(files)+'.pkl','wb') as f:\n",
    "            pickle.dump(vdist,f, pickle.HIGHEST_PROTOCOL)\n",
    "        # we reset our variable and counter\n",
    "        vdist = dict()\n",
    "        vx = 0\n",
    "\n",
    "with open('dist_dict_'+str(files+1)+'.pkl','wb') as f:\n",
    "            pickle.dump(vdist,f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the file and proceed to get the ordered category list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:34:10.857260Z",
     "start_time": "2018-12-14T16:31:45.795309Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]\n",
      "  3%|██▎                                                                                | 1/35 [01:23<47:34, 83.96s/it]\n",
      "  6%|████▋                                                                              | 2/35 [02:42<45:19, 82.42s/it]\n",
      "  9%|███████                                                                            | 3/35 [03:59<43:04, 80.77s/it]\n",
      " 11%|█████████▍                                                                         | 4/35 [05:19<41:32, 80.40s/it]\n",
      " 14%|███████████▊                                                                       | 5/35 [06:46<41:10, 82.35s/it]\n",
      " 17%|██████████████▏                                                                    | 6/35 [08:14<40:37, 84.05s/it]\n",
      " 20%|████████████████                                                                | 7/35 [17:13<1:42:55, 220.55s/it]\n",
      " 23%|██████████████████▎                                                             | 8/35 [18:34<1:20:28, 178.85s/it]\n",
      " 26%|████████████████████▌                                                           | 9/35 [19:57<1:04:56, 149.88s/it]\n",
      " 29%|███████████████████████▏                                                         | 10/35 [21:14<53:20, 128.00s/it]\n",
      " 31%|█████████████████████████▍                                                       | 11/35 [22:33<45:19, 113.33s/it]\n",
      " 34%|███████████████████████████▊                                                     | 12/35 [23:51<39:24, 102.81s/it]\n",
      " 37%|██████████████████████████████▍                                                   | 13/35 [25:09<34:57, 95.35s/it]\n",
      " 40%|████████████████████████████████▊                                                 | 14/35 [26:23<31:11, 89.12s/it]\n",
      " 43%|███████████████████████████████████▏                                              | 15/35 [28:21<32:33, 97.65s/it]\n",
      " 46%|█████████████████████████████████████▍                                            | 16/35 [29:42<29:19, 92.61s/it]\n",
      " 49%|███████████████████████████████████████▊                                          | 17/35 [31:04<26:49, 89.44s/it]\n",
      " 51%|██████████████████████████████████████████▏                                       | 18/35 [32:21<24:16, 85.70s/it]\n",
      " 54%|████████████████████████████████████████████▌                                     | 19/35 [33:45<22:42, 85.13s/it]\n",
      " 57%|██████████████████████████████████████████████▊                                   | 20/35 [35:00<20:33, 82.24s/it]\n",
      " 60%|█████████████████████████████████████████████████▏                                | 21/35 [36:17<18:47, 80.51s/it]\n",
      " 63%|███████████████████████████████████████████████████▌                              | 22/35 [38:00<18:55, 87.33s/it]\n",
      " 66%|█████████████████████████████████████████████████████▉                            | 23/35 [39:34<17:51, 89.29s/it]\n",
      " 69%|████████████████████████████████████████████████████████▏                         | 24/35 [40:49<15:36, 85.10s/it]\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 25/35 [42:08<13:52, 83.26s/it]\n",
      " 74%|████████████████████████████████████████████████████████████▉                     | 26/35 [43:25<12:12, 81.36s/it]\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 27/35 [44:53<11:06, 83.35s/it]\n",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 28/35 [46:25<10:00, 85.85s/it]\n",
      " 83%|███████████████████████████████████████████████████████████████████▉              | 29/35 [47:47<08:28, 84.71s/it]\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 30/35 [49:13<07:05, 85.10s/it]\n",
      " 89%|████████████████████████████████████████████████████████████████████████▋         | 31/35 [50:29<05:29, 82.49s/it]\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▉       | 32/35 [51:49<04:05, 81.74s/it]\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▎    | 33/35 [53:05<02:40, 80.12s/it]\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 34/35 [54:21<01:18, 78.63s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [55:38<00:00, 78.28s/it]\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "files = int(len(category_dic[input_category])/100)+1\n",
    "# we get the distance from the input category to each other\n",
    "for category in tqdm(category_dic.keys()):\n",
    "    infty_c = 0\n",
    "    dist_list = []\n",
    "    # we iterate on the file contaning the distances\n",
    "    for i in range(1,files+1):\n",
    "        with open('dist_dict_'+str(i)+'.pkl','rb') as f:\n",
    "            vdist = pickle.load(f)\n",
    "        for v in vdist.keys():\n",
    "            for w in category_dic[category]:\n",
    "                d = vdist[v][vertex_to_index[w]]\n",
    "                if d == np.infty:\n",
    "                    infty_c+=1\n",
    "                else:\n",
    "                    dist_list.append(d)\n",
    "    dist_list = sorted(dist_list)\n",
    "    s = len(dist_list)+infty_c\n",
    "    try:\n",
    "        if s%2 == 0:\n",
    "            l.append((category,(dist_list[int(s/2)]+dist_list[int(s/2)-1])/2))\n",
    "        else:\n",
    "            l.append((category,dist_list[int((s-1)/2)]))\n",
    "    except IndexError:\n",
    "        l.append((category,np.infty))\n",
    "    # and we store them in a list\n",
    "    l.append((category,np.median(dist_list)))\n",
    "# we save the ordered list\n",
    "ordered_cat = sorted(l,key= lambda x:x[1])\n",
    "\n",
    "# we move the input category to the first position and assign to him weight 0\n",
    "ordered_cat.pop(list(map(lambda x: x[0],ordered_cat)).index(input_category))\n",
    "ordered_cat.insert(0,(input_category,0))\n",
    "\n",
    "# we save our newly found item\n",
    "with open('olist_cat.pkl','wb') as f:\n",
    "    pickle.dump(ordered_cat,f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from our code, we decided to consider the median of the values on each couple of point, even if they are not linked. In this way we still consider that two categories are far away if they have any element in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:43:50.385941Z",
     "start_time": "2018-12-14T20:43:50.326102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year_of_birth_unknown 0\n",
      "Harvard_University_alumni 6.0\n",
      "English_television_actors 6.0\n",
      "British_films 6.0\n",
      "English-language_films 6.0\n",
      "English-language_films 6.0\n",
      "American_films 6.0\n",
      "Fellows_of_the_Royal_Society 6.0\n",
      "People_from_New_York_City 6.0\n",
      "American_Jews 6.0\n",
      "American_television_actors 6.0\n",
      "American_film_actors 6.0\n",
      "American_film_actors 6.0\n",
      "Black-and-white_films 6.0\n",
      "Article_Feedback_Pilot 6.0\n",
      "American_military_personnel_of_World_War_II 6.0\n",
      "English_footballers 7.0\n",
      "The_Football_League_players 7.0\n",
      "Association_football_forwards 7.0\n",
      "Association_football_goalkeepers 7.0\n",
      "Association_football_midfielders 7.0\n",
      "Association_football_defenders 7.0\n",
      "Living_people 7.0\n",
      "Major_League_Baseball_pitchers 7.0\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies 7.0\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies 7.0\n",
      "Indian_films 7.0\n",
      "Indian_films 7.0\n",
      "Year_of_death_missing 7.0\n",
      "English_cricketers 7.0\n",
      "Year_of_birth_missing_(living_people) 7.0\n",
      "Rivers_of_Romania 7.0\n",
      "English-language_albums 7.0\n",
      "English_television_actors 7.0\n",
      "British_films 7.0\n",
      "American_films 7.0\n",
      "American_Jews 7.0\n",
      "American_television_actors 7.0\n",
      "Debut_albums 7.0\n",
      "Black-and-white_films 7.0\n",
      "Year_of_birth_missing 7.0\n",
      "Place_of_birth_missing_(living_people) 7.0\n",
      "Article_Feedback_Pilot 7.0\n",
      "Windows_games 7.0\n",
      "Rivers_of_Romania 8.0\n",
      "Asteroids_named_for_people 8.0\n",
      "English-language_albums 8.0\n",
      "People_from_New_York_City 8.0\n",
      "Main_Belt_asteroids 9.0\n",
      "Debut_albums 9.0\n",
      "English_footballers Infinity\n",
      "The_Football_League_players Infinity\n",
      "Association_football_forwards Infinity\n",
      "Association_football_goalkeepers Infinity\n",
      "Association_football_midfielders Infinity\n",
      "Association_football_defenders Infinity\n",
      "Living_people Infinity\n",
      "Year_of_birth_unknown Infinity\n",
      "Harvard_University_alumni Infinity\n",
      "Major_League_Baseball_pitchers Infinity\n",
      "Year_of_death_missing Infinity\n",
      "English_cricketers Infinity\n",
      "Year_of_birth_missing_(living_people) Infinity\n",
      "Main_Belt_asteroids Infinity\n",
      "Asteroids_named_for_people Infinity\n",
      "Fellows_of_the_Royal_Society Infinity\n",
      "Year_of_birth_missing Infinity\n",
      "Place_of_birth_missing_(living_people) Infinity\n",
      "American_military_personnel_of_World_War_II Infinity\n",
      "Windows_games Infinity\n"
     ]
    }
   ],
   "source": [
    "with open('olist_cat.pkl','rb') as f:\n",
    "    ordered_cat = pickle.load(f)\n",
    "\n",
    "for item in ordered_cat:\n",
    "    print(item[0],item[1] if item[1] != np.infty else 'Infinity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute which vertex are in our blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:53:10.265087Z",
     "start_time": "2018-12-14T20:53:10.132411Z"
    }
   },
   "outputs": [],
   "source": [
    "# we make a list of vertex to assign\n",
    "unassigned_vertex = total_vertex.copy()\n",
    "\n",
    "block_list = []\n",
    "# we populate each category\n",
    "for cat in ordered_cat:\n",
    "    # we check if we still have verteces to assign\n",
    "    if not unassigned_vertex:\n",
    "        break\n",
    "    # and we assign all the still free verteces to his category\n",
    "    else:\n",
    "        to_insert = unassigned_vertex & category_dic[cat[0]]\n",
    "        block_list.append((cat[0],to_insert))\n",
    "        # we delete from our set of free verteces the asigned ones\n",
    "        unassigned_vertex-=to_insert\n",
    "# we delete categories that did not get any vertex\n",
    "tlist = []\n",
    "for item in block_list:\n",
    "    if item[1]:\n",
    "        tlist.append(item)\n",
    "block_list = tlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give a look to which blocks we have and how big they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:53:12.706402Z",
     "start_time": "2018-12-14T20:53:12.606668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year_of_birth_unknown 2536\n",
      "Harvard_University_alumni 5544\n",
      "English_television_actors 3359\n",
      "British_films 4422\n",
      "English-language_films 18909\n",
      "American_films 4598\n",
      "Fellows_of_the_Royal_Society 3425\n",
      "People_from_New_York_City 4432\n",
      "American_Jews 2909\n",
      "American_television_actors 10495\n",
      "American_film_actors 4587\n",
      "Black-and-white_films 3620\n",
      "Article_Feedback_Pilot 3398\n",
      "American_military_personnel_of_World_War_II 3183\n",
      "English_footballers 7514\n",
      "The_Football_League_players 2790\n",
      "Association_football_forwards 3366\n",
      "Association_football_goalkeepers 2837\n",
      "Association_football_midfielders 4009\n",
      "Association_football_defenders 2896\n",
      "Living_people 306928\n",
      "Major_League_Baseball_pitchers 1931\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies 5024\n",
      "Indian_films 5399\n",
      "Year_of_death_missing 3544\n",
      "English_cricketers 1820\n",
      "Year_of_birth_missing_(living_people) 74\n",
      "Rivers_of_Romania 7728\n",
      "English-language_albums 4746\n",
      "Debut_albums 6647\n",
      "Year_of_birth_missing 2451\n",
      "Place_of_birth_missing_(living_people) 29\n",
      "Windows_games 4012\n",
      "Asteroids_named_for_people 4893\n",
      "Main_Belt_asteroids 7123\n"
     ]
    }
   ],
   "source": [
    "for item in block_list:\n",
    "    print(item[0],len(item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the reversed graph, that is, to each key is associate the entering edge, not the exiting one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:53:20.495509Z",
     "start_time": "2018-12-14T20:53:16.448160Z"
    }
   },
   "outputs": [],
   "source": [
    "entring_edge = defaultdict(list)\n",
    "\n",
    "with open('wiki-topcats-reduced.txt') as f:\n",
    "    for row in f.readlines():\n",
    "        edges = list(map(int,row.split('\\t')))\n",
    "        entring_edge[edges[1]].append(edges[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We associate to each veretex a weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:53:22.178809Z",
     "start_time": "2018-12-14T20:53:20.497293Z"
    }
   },
   "outputs": [],
   "source": [
    "# we inizialize a dictionary to be populated with as keys our vertex, and as value their weight\n",
    "w_dic = defaultdict(int)\n",
    "# we have a set of all the vertex in the categories we are considering up to now\n",
    "considering = set()\n",
    "for i in range(len(block_list)):\n",
    "    # we upload the considering set with all the new vertex\n",
    "    considering|=block_list[i][1]\n",
    "    # for each element in our block\n",
    "    for v in block_list[i][1]:\n",
    "        # and for each considered element that has an entering edge in our element\n",
    "        for w in considering & set(entring_edge[v]):\n",
    "                # we appload his value\n",
    "                if w in block_list[i][1]:\n",
    "                    w_dic[v]+=1\n",
    "                else:\n",
    "                    w_dic[v]+=w_dic[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote an algorithm to show you the correlation between the first two blocks using igraph.plot. The size of the images are really big, and we can not not load them inside this notebook. We saved them in the svg format and loaded them in the github repository. As the svg format is naturally readible in a browser we placed the link to the visualization of the file.\n",
    "\n",
    "We display in different colors different categories, and place inside each vertex his total weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:53:22.185829Z",
     "start_time": "2018-12-14T20:53:22.180850Z"
    }
   },
   "outputs": [],
   "source": [
    "# we set how many block we want to plot\n",
    "n_block = 2\n",
    "# the size of the plot\n",
    "size = 10000\n",
    "\n",
    "# we get a subset of our total list of vertex corresponding to this subgraph\n",
    "to_plot = {}\n",
    "our_vert = set()\n",
    "for i in range(n_block):\n",
    "    our_vert|=block_list[i][1]\n",
    "for k in our_vert:\n",
    "    to_plot[k] = set(graph[k]) & our_vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-14T20:47:44.954Z"
    }
   },
   "outputs": [],
   "source": [
    "# we create an empty graph\n",
    "g = igraph.Graph(directed = True)\n",
    "# we add the vertices\n",
    "g.add_vertices(list(map(str,to_plot.keys())))\n",
    "# the edges\n",
    "g.add_edges([(str(s),str(g))  for s in to_plot.keys() for g in to_plot[s]])\n",
    "# we place as label the weight of each vertix\n",
    "g.vs['label'] = [w_dic[int(k)] for k in g.vs['name']]\n",
    "# we prepare a list with as many color as blocks plotted\n",
    "colorlist = ['red', 'blue']\n",
    "# and we associate each color to the vertex of a block\n",
    "vertex_color = []\n",
    "for k in g.vs['name']:\n",
    "    for i in range(n_block):\n",
    "        if int(k) in block_list[i][1]:\n",
    "            vertex_color.append(colorlist[i])\n",
    "            break\n",
    "g.vs['color'] = vertex_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.write_svg('plot1.svg',width= size, height= size, vertex_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CLICK HERE TO CHECK OUR FIRST PLOT !](https://raw.githubusercontent.com/CervelliRic/ADM-HW5/master/plot1.svg?sanitize=true)\n",
    "\n",
    "Careful, the image is very big, you will need to move the point of view because at first it will be focused in the top left corener, potentially empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we prepare the layout\n",
    "lay = g.layout_lgl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.write_svg('plot2.svg',width= size, height= size, vertex_size=7,layout=lay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AND HERE TO CHECK OUR SECOND PLOT !](https://raw.githubusercontent.com/CervelliRic/ADM-HW5/master/plot2.svg?sanitize=true)\n",
    "\n",
    "Careful, the image is very big, you will need to move the point of view because at first it will be focused in the top left corener, potentially empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is not very informative what we obtained because the graph is huge, but we can still see something.\n",
    "\n",
    "We plotted two graphs. The first highlight how many unlinked vertex we have obtain.With the second let us see in detail the weight of the verteces in the middle of the first one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice there are a lot of verteces with weight $0$ (it is also implied by the low density). We decided to plot only vertex with positive weight, more interesting for us and certainly we can have a better the idea of what we are observing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot2 = {}\n",
    "\n",
    "for k in to_plot.keys():\n",
    "    if w_dic[k] > 0:\n",
    "        to_plot2[k] = [e for e in to_plot[k] if w_dic[e] > 0]\n",
    "\n",
    "# we create an empty graph\n",
    "g = igraph.Graph(directed = True)\n",
    "# we add the vertices\n",
    "g.add_vertices(list(map(str,to_plot2.keys())))\n",
    "# the edges\n",
    "g.add_edges([(str(s),str(g))  for s in to_plot2.keys() for g in to_plot2[s]])\n",
    "# we place as label the weight of each vertix\n",
    "g.vs['label'] = [w_dic[int(k)] for k in g.vs['name']]\n",
    "# we prepare a list with as many color as blocks plotted\n",
    "colorlist = ['red', 'blue']\n",
    "# and we associate each color to the vertex of a block\n",
    "vertex_color = []\n",
    "for k in g.vs['name']:\n",
    "    for i in range(n_block):\n",
    "        if int(k) in block_list[i][1]:\n",
    "            vertex_color.append(colorlist[i])\n",
    "            break\n",
    "g.vs['color'] = vertex_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.write_svg('redplot1.svg',width= size, height= size, vertex_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HERE WE ARE PLOTTING ONLY THE VERTEX WITH POSITIVE WEIGHT.](https://raw.githubusercontent.com/CervelliRic/ADM-HW5/master/redplot1.svg?sanitize=true)\n",
    "\n",
    "Careful, the image is very big, you will need to move the point of view because at first it will be focused in the top left corener, potentially empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we prepare the layout\n",
    "lay = g.layout_lgl()\n",
    "\n",
    "g.write_svg('redplot2.svg',width= size, height= size, vertex_size=7, layout=lay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[POSITIVE WEIGHT OF THE VERTECES IN THE MIDDLE.](https://raw.githubusercontent.com/CervelliRic/ADM-HW5/master/redplot2.svg?sanitize=true)\n",
    "\n",
    "Careful, the image is very big, you will need to move the point of view because at first it will be focused in the top left corener, potentially empty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
