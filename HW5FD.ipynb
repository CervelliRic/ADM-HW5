{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Visit the Wikipedia hyperlinks graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group #2 Riccardo Cervelli, Davide Facchinelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WIKI](https://www.balcanicaucaso.org/var/obc/storage/images/articoli-da-pubblicare-2/turchia-oscurata-wikipedia/1221571-1-ita-IT/Turchia-oscurata-Wikipedia.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import statistics\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import igraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Goal of this assignment is to perform an analysis of the Wikipedia Hyperlink graph. In particular, given extra information about the categories to which an article belongs to, we ranked the articles according to some criteria that we'll explain you later.\n",
    "For this purpose we use the Wikipedia graph released by the SNAP group and you can check the files that we used [here](https://drive.google.com/file/d/1ghPJ4g6XMCUDFQ2JPqAVveLyytG8gBfL/view?usp=sharing), in order to compute find:\n",
    "- The number of nodes.\n",
    "- The number of edges.\n",
    "- The average node degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are building our graph reading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:32.836315Z",
     "start_time": "2018-12-14T20:52:29.303717Z"
    }
   },
   "outputs": [],
   "source": [
    "# we inizialize our graph as a default dictionary\n",
    "graph = defaultdict(list)\n",
    "\n",
    "# and populate it from our file\n",
    "with open('wiki-topcats-reduced.txt') as f:\n",
    "    for row in f.readlines():\n",
    "        edges = list(map(int,row.split('\\t')))\n",
    "        graph[edges[0]].append(edges[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are building the set of all the vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.347548Z",
     "start_time": "2018-12-14T20:52:32.957163Z"
    }
   },
   "outputs": [],
   "source": [
    "total_vertex = list(graph.keys())\n",
    "for k in graph.keys():\n",
    "    total_vertex+=graph[k]\n",
    "total_vertex = set(total_vertex)\n",
    "indicizer = sorted(list(total_vertex))\n",
    "\n",
    "# we prepare a dictonary that given a vertex it gives us it's index\n",
    "vertex_to_index = {indicizer[i]:i for i in range(len(indicizer))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the Number of vertex :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.351438Z",
     "start_time": "2018-12-14T20:52:34.348444Z"
    }
   },
   "outputs": [],
   "source": [
    "n_vert = len(total_vertex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate  Number of edges :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.794360Z",
     "start_time": "2018-12-14T20:52:34.352436Z"
    }
   },
   "outputs": [],
   "source": [
    "n_edges = sum(len(graph[k]) for k in total_vertex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the Density of our graph ($D$) :\n",
    "$D ={\\frac  {n_v}{n_v-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.798244Z",
     "start_time": "2018-12-14T20:52:34.795250Z"
    }
   },
   "outputs": [],
   "source": [
    "density = n_edges/(n_vert*(n_vert-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the Avreage node degree ($AND$) :  $AND ={\\frac  {2n_e}{n_v}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.816296Z",
     "start_time": "2018-12-14T20:52:34.799241Z"
    }
   },
   "outputs": [],
   "source": [
    "avreage_degree = 2*n_edges/n_vert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:34.835147Z",
     "start_time": "2018-12-14T20:52:34.818189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of vertex is :  461193\n",
      "The Number of edges is : 2645247\n",
      "The density of our graph is : 1.2436602635647606e-05\n",
      "The Average node Degree is : 11.471323285479182\n"
     ]
    }
   ],
   "source": [
    "print('The Number of vertex is : ',n_vert)\n",
    "print('The Number of edges is :',n_edges)\n",
    "print('The density of our graph is :',density)\n",
    "print('The Average node Degree is :',avreage_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First question : Is the graph dense?\n",
    "In mathematics, a dense graph is a graph in which the number of edges is close to the maximal number of edges. We calculated the density as $D={\\frac  {|E|}{|V|\\,(|V|-1)}}$ and we can see that is quite low. So no, the graph is not dense. We will show you later our plots, where yourselfe can observe the density of our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go to the next step.\n",
    "- Load the categories (take into account all the categories that have a number of articles greater than 3500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:36.696674Z",
     "start_time": "2018-12-14T20:52:35.437713Z"
    }
   },
   "outputs": [],
   "source": [
    "# we inizialize it as an empty dictonary\n",
    "category_dic = {}\n",
    "\n",
    "# and load the category from the file\n",
    "with open('wiki-topcats-categories.txt') as f:\n",
    "    for row in f.readlines():\n",
    "        category = row[:-2].split(' ')\n",
    "        elements = set(map(int,category[1:]))\n",
    "        # checking if the category is big enough\n",
    "        if len(elements)>3500:\n",
    "            # and taking only verteces that exist in our starting graph\n",
    "            category_dic[category[0][9:-1]] = elements & total_vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:36.701600Z",
     "start_time": "2018-12-14T20:52:36.697658Z"
    }
   },
   "outputs": [],
   "source": [
    "# we create a temporary second dictonary\n",
    "category_dic2 = {}\n",
    "# and we save in it only the category that are not empty\n",
    "for k in category_dic.keys():\n",
    "    if category_dic[k]:\n",
    "        category_dic2[k] = category_dic[k]\n",
    "category_dic = category_dic2\n",
    "del category_dic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:36.721586Z",
     "start_time": "2018-12-14T20:52:36.702597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can use only 35 categories.\n"
     ]
    }
   ],
   "source": [
    "print('We can use only',len(category_dic),'categories.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how big these categories are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:38.124791Z",
     "start_time": "2018-12-14T20:52:38.007106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English_footballers 7537\n",
      "The_Football_League_players 7813\n",
      "Association_football_forwards 5097\n",
      "Association_football_goalkeepers 3736\n",
      "Association_football_midfielders 5826\n",
      "Association_football_defenders 4589\n",
      "Living_people 348299\n",
      "Year_of_birth_unknown 2536\n",
      "Harvard_University_alumni 5548\n",
      "Major_League_Baseball_pitchers 5193\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies 6491\n",
      "Indian_films 5567\n",
      "Year_of_death_missing 4122\n",
      "English_cricketers 3275\n",
      "Year_of_birth_missing_(living_people) 28498\n",
      "Rivers_of_Romania 7728\n",
      "Main_Belt_asteroids 11659\n",
      "Asteroids_named_for_people 4895\n",
      "English-language_albums 4760\n",
      "English_television_actors 3361\n",
      "British_films 4422\n",
      "English-language_films 22462\n",
      "American_films 15158\n",
      "Fellows_of_the_Royal_Society 3446\n",
      "People_from_New_York_City 4615\n",
      "American_Jews 3411\n",
      "American_television_actors 11531\n",
      "American_film_actors 13865\n",
      "Debut_albums 7561\n",
      "Black-and-white_films 10758\n",
      "Year_of_birth_missing 4346\n",
      "Place_of_birth_missing_(living_people) 5532\n",
      "Article_Feedback_Pilot 3485\n",
      "American_military_personnel_of_World_War_II 3720\n",
      "Windows_games 4025\n"
     ]
    }
   ],
   "source": [
    "for item in category_dic.items():\n",
    "    print(item[0],len(item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a function from our file we get the list of categories ordered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define  a BFS function implemented from scratch ,finding all the shortest distance from a vertex to all the others.\n",
    "Notice that if there is not a way to go from a node to another we assigned as distance the value $-1$. Classically in the literature the value assigned in those cases is $\\infty$, and further in the code we will substitue the $-1$ value with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:52:41.832466Z",
     "start_time": "2018-12-14T20:52:41.827479Z"
    }
   },
   "outputs": [],
   "source": [
    "# calssic BFS algorithm\n",
    "\n",
    "def path_len(start,graph,indicizer):\n",
    "    \n",
    "    verteces = defaultdict(lambda:(False,-1))\n",
    "    verteces[start] = (True,0)\n",
    "    \n",
    "    queue = [start]\n",
    "\n",
    "    while queue:\n",
    "        actual = queue.pop(0)\n",
    "        d = verteces[actual][1]\n",
    "        for child in graph[actual]:\n",
    "            if not verteces[child][0]:\n",
    "                verteces[child] = (True,d+1)\n",
    "                queue.append(child)\n",
    "    result = []\n",
    "    for k in indicizer:\n",
    "        result.append(verteces[k][1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we input our input category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T11:49:21.879935Z",
     "start_time": "2018-12-14T11:49:20.816688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input category: Year_of_birth_unknown\n"
     ]
    }
   ],
   "source": [
    "input_category = input('Input category: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get all the distances from the vertex of the input category to all the other vertices. As the files are really big we save them on the drive because we tryed to don't overload the RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T11:32:31.446300Z",
     "start_time": "2018-12-14T08:29:00.217074Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2536/2536 [3:03:31<00:00,  1.88s/it]  \n"
     ]
    }
   ],
   "source": [
    "# we apply the BFS algorithm to each element of the input_category\n",
    "vdist = dict()\n",
    "# we inzialize two counters\n",
    "# one to count how many start vertex we have stored\n",
    "vx = 0\n",
    "# the other will set the name of the file where we are going to save them\n",
    "files = 0\n",
    "for v in tqdm(category_dic[input_category]):\n",
    "    # we compure the distance from v to each other vertex\n",
    "    vdist[v] = path_len(v,graph,indicizer)\n",
    "    vx+=1\n",
    "    # and each 100 vertex we save it on a file\n",
    "    if vx == 100:\n",
    "        files+=1\n",
    "        with open('dist_dict_'+str(files)+'.pkl','wb') as f:\n",
    "            pickle.dump(vdist,f, pickle.HIGHEST_PROTOCOL)\n",
    "        # we reset our variable and counter\n",
    "        vdist = dict()\n",
    "        vx = 0\n",
    "\n",
    "with open('dist_dict_'+str(files+1)+'.pkl','wb') as f:\n",
    "            pickle.dump(vdist,f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the file and proceed to get the ordered category list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:34:10.857260Z",
     "start_time": "2018-12-14T16:31:45.795309Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [4:02:24<00:00, 382.33s/it]  \n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "files = int(len(category_dic[input_category])/100)+1\n",
    "# we get the distance from the input category to each other\n",
    "for category in tqdm(category_dic.keys()):\n",
    "    dist_list = []\n",
    "    # we iterate on the file contaning the distances\n",
    "    for i in range(1,files+1):\n",
    "        with open('dist_dict_'+str(i)+'.pkl','rb') as f:\n",
    "            vdist = pickle.load(f)\n",
    "        for v in vdist.keys():\n",
    "            for w in category_dic[category]:\n",
    "                dist_list.append(vdist[v][vertex_to_index[w]])\n",
    "    try:\n",
    "        # and we store them in a list\n",
    "        l.append((category,statistics.median([np.infty if d < 0 else d for d in dist_list])))\n",
    "    except statistics.StatisticsError:\n",
    "        pass\n",
    "# we save the ordered list\n",
    "ordered_cat = sorted(l,key= lambda x:x[1])\n",
    "\n",
    "# we free some space as vdist is a big item\n",
    "del vdist\n",
    "\n",
    "# we save our newly found item\n",
    "with open('olist_cat.pkl','wb') as f:\n",
    "    pickle.dump(ordered_cat,f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from our code, we decided to consider the median of the values on each couple of point, even if they are not linked. In this way we still consider that two categories are far away if they have any element in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:43:50.385941Z",
     "start_time": "2018-12-14T20:43:50.326102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English-language_films 6.0\n",
      "American_film_actors 6.0\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies 7.0\n",
      "Indian_films 7.0\n",
      "English_television_actors 7.0\n",
      "British_films 7.0\n",
      "American_films 7.0\n",
      "American_Jews 7.0\n",
      "American_television_actors 7.0\n",
      "Black-and-white_films 7.0\n",
      "Article_Feedback_Pilot 7.0\n",
      "Rivers_of_Romania 8.0\n",
      "English-language_albums 8.0\n",
      "People_from_New_York_City 8.0\n",
      "Debut_albums 9.0\n",
      "English_footballers Infinity\n",
      "The_Football_League_players Infinity\n",
      "Association_football_forwards Infinity\n",
      "Association_football_goalkeepers Infinity\n",
      "Association_football_midfielders Infinity\n",
      "Association_football_defenders Infinity\n",
      "Living_people Infinity\n",
      "Year_of_birth_unknown Infinity\n",
      "Harvard_University_alumni Infinity\n",
      "Major_League_Baseball_pitchers Infinity\n",
      "Year_of_death_missing Infinity\n",
      "English_cricketers Infinity\n",
      "Year_of_birth_missing_(living_people) Infinity\n",
      "Main_Belt_asteroids Infinity\n",
      "Asteroids_named_for_people Infinity\n",
      "Fellows_of_the_Royal_Society Infinity\n",
      "Year_of_birth_missing Infinity\n",
      "Place_of_birth_missing_(living_people) Infinity\n",
      "American_military_personnel_of_World_War_II Infinity\n",
      "Windows_games Infinity\n"
     ]
    }
   ],
   "source": [
    "for item in ordered_cat:\n",
    "    print(item[0],item[1] if item[1] != np.infty else 'Infinity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute which vertex are in our blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:53:10.265087Z",
     "start_time": "2018-12-14T20:53:10.132411Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('olist_cat.pkl','rb') as f:\n",
    "    ordered_cat = pickle.load(f)\n",
    "\n",
    "# we make a list of vertex to assign\n",
    "unassigned_vertex = total_vertex.copy()\n",
    "\n",
    "block_list = []\n",
    "# we populate each category\n",
    "for cat in ordered_cat:\n",
    "    # we check if we still have verteces to assign\n",
    "    if not unassigned_vertex:\n",
    "        break\n",
    "    # and we assign all the still free verteces to his category\n",
    "    else:\n",
    "        to_insert = unassigned_vertex & category_dic[cat[0]]\n",
    "        block_list.append((cat[0],to_insert))\n",
    "        # we delete from our set of free verteces the asigned ones\n",
    "        unassigned_vertex-=to_insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give a look to which blocks we have and how big they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:53:12.706402Z",
     "start_time": "2018-12-14T20:53:12.606668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English-language_films 22462\n",
      "American_film_actors 13865\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies 6491\n",
      "Indian_films 5537\n",
      "English_television_actors 3326\n",
      "British_films 868\n",
      "American_films 4596\n",
      "American_Jews 2907\n",
      "American_television_actors 2505\n",
      "Black-and-white_films 3489\n",
      "Article_Feedback_Pilot 3414\n",
      "Rivers_of_Romania 7728\n",
      "English-language_albums 4747\n",
      "People_from_New_York_City 3583\n",
      "Debut_albums 6649\n",
      "English_footballers 7515\n",
      "The_Football_League_players 2794\n",
      "Association_football_forwards 3367\n",
      "Association_football_goalkeepers 2838\n",
      "Association_football_midfielders 4010\n",
      "Association_football_defenders 2896\n",
      "Living_people 309499\n",
      "Year_of_birth_unknown 2372\n",
      "Harvard_University_alumni 2345\n",
      "Major_League_Baseball_pitchers 1984\n",
      "Year_of_death_missing 3557\n",
      "English_cricketers 1829\n",
      "Year_of_birth_missing_(living_people) 74\n",
      "Main_Belt_asteroids 11658\n",
      "Asteroids_named_for_people 358\n",
      "Fellows_of_the_Royal_Society 2693\n",
      "Year_of_birth_missing 2468\n",
      "Place_of_birth_missing_(living_people) 29\n",
      "American_military_personnel_of_World_War_II 2713\n",
      "Windows_games 4012\n"
     ]
    }
   ],
   "source": [
    "for item in block_list:\n",
    "    print(item[0],len(item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the reversed graph, that is, to each key is associate the entering edge, not the exiting one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:53:20.495509Z",
     "start_time": "2018-12-14T20:53:16.448160Z"
    }
   },
   "outputs": [],
   "source": [
    "entring_edge = defaultdict(list)\n",
    "\n",
    "with open('wiki-topcats-reduced.txt') as f:\n",
    "    for row in f.readlines():\n",
    "        edges = list(map(int,row.split('\\t')))\n",
    "        entring_edge[edges[1]].append(edges[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We associate to each veretex a weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:53:22.178809Z",
     "start_time": "2018-12-14T20:53:20.497293Z"
    }
   },
   "outputs": [],
   "source": [
    "# we inizialize a dictionary to be populated with as keys our vertex, and as value their weight\n",
    "w_dic = defaultdict(int)\n",
    "# we have a set of all the vertex in the categories we are considering up to now\n",
    "considering = set()\n",
    "for i in range(len(block_list)):\n",
    "    # we upload the considering set with all the new vertex\n",
    "    considering|=block_list[i][1]\n",
    "    # for each element in our block\n",
    "    for v in block_list[i][1]:\n",
    "        # and for each considered element that has an entering edge in our element\n",
    "        for w in considering & set(entring_edge[v]):\n",
    "                # we appload his value\n",
    "                if w in block_list[i][1]:\n",
    "                    w_dic[v]+=1\n",
    "                else:\n",
    "                    w_dic[v]+=w_dic[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote an algorithm to show you the correlation between the first two blocks using igraph.plot. The size of the images are really big, and we can not not load them inside this file, therefore we placed direct links for each of these, and you can also view them in our GitHub repository.\n",
    "\n",
    "We display in different colors different categories, and place inside each vertex his total weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T20:53:22.185829Z",
     "start_time": "2018-12-14T20:53:22.180850Z"
    }
   },
   "outputs": [],
   "source": [
    "# we set how many block we want to plot\n",
    "n_block = 2\n",
    "\n",
    "# we get a subset of our total list of vertex corresponding to this subgraph\n",
    "to_plot = {}\n",
    "our_vert = set()\n",
    "for i in range(n_block):\n",
    "    our_vert|=block_list[i][1]\n",
    "for k in our_vert:\n",
    "    to_plot[k] = set(graph[k]) & our_vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-14T20:47:44.954Z"
    }
   },
   "outputs": [],
   "source": [
    "# we create an empty graph\n",
    "g = igraph.Graph()\n",
    "# we add the vertices\n",
    "g.add_vertices(list(map(str,to_plot.keys())))\n",
    "# the edges\n",
    "g.add_edges([(str(s),str(g))  for s in to_plot.keys() for g in to_plot[s]])\n",
    "# we place as label the weight of each vertix\n",
    "g.vs['label'] = [w_dic[int(k)] for k in g.vs['name']]\n",
    "# we prepare a list with as many color as blocks plotted\n",
    "colorlist = ['red', 'blue']\n",
    "# and we associate each color to the vertex of a block\n",
    "vertex_color = []\n",
    "for k in g.vs['name']:\n",
    "    for i in range(n_block):\n",
    "        if int(k) in block_list[i][1]:\n",
    "            vertex_color.append(colorlist[i])\n",
    "            break\n",
    "g.vs['color'] = vertex_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot\n",
    "plt = igraph.plot(g,vertex_size = 15,bbox = (6000,6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and save it as, do to his weight, it is tricky to visualize inside the notebook\n",
    "plt.save('plot1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CLICK HERE TO CHECK OUR FIRST PLOT !](https://www.flickr.com/photos/166751643@N04/45599674394/in/dateposted-public/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we prepare the layout\n",
    "lay = g.layout_lgl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot\n",
    "plt = igraph.plot(g, layout = lay, vertex_size = 15,bbox = (6000,6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and save it as, do to his weight, it is tricky to visualize inside the notebook\n",
    "plt.save('plot2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AND HERE TO CHECK OUR SECOND PLOT !](https://www.flickr.com/photos/166751643@N04/31384085647/in/dateposted-public/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is not very informative what we obtained because the graph is huge, but we can still see something.\n",
    "\n",
    "We plotted two graphs. The first highlight how many unlinked vertex we have obtain.With the second let us see in detail the weight of the verteces in the middle of the first one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can notice there are a lot of verteces with weight $0$, so we decided to plot only vertex with positive weight, more interesting for us and certainly we can have a better the idea of what we are observing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot2 = {}\n",
    "\n",
    "for k in to_plot.keys():\n",
    "    if w_dic[k] > 0:\n",
    "        to_plot2[k] = [e for e in to_plot[k] if w_dic[e] > 0]\n",
    "\n",
    "# we create an empty graph\n",
    "g = igraph.Graph()\n",
    "# we add the vertices\n",
    "g.add_vertices(list(map(str,to_plot2.keys())))\n",
    "# the edges\n",
    "g.add_edges([(str(s),str(g))  for s in to_plot2.keys() for g in to_plot2[s]])\n",
    "# we place as label the weight of each vertix\n",
    "g.vs['label'] = [w_dic[int(k)] for k in g.vs['name']]\n",
    "# we prepare a list with as many color as blocks plotted\n",
    "colorlist = ['red', 'blue']\n",
    "# and we associate each color to the vertex of a block\n",
    "vertex_color = []\n",
    "for k in g.vs['name']:\n",
    "    for i in range(n_block):\n",
    "        if int(k) in block_list[i][1]:\n",
    "            vertex_color.append(colorlist[i])\n",
    "            break\n",
    "g.vs['color'] = vertex_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot\n",
    "plt = igraph.plot(g,vertex_size = 15,bbox = (6000,6000))\n",
    "# and save it as, do to his weight, it is tricky to visualize inside the notebook\n",
    "plt.save('redplot1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HERE WE ARE PLOTTING ONLY THE VERTEX WITH POSITIVE WEIGHT.](https://www.flickr.com/photos/166751643@N04/31384084987/in/dateposted-public/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we prepare the layout\n",
    "lay = g.layout_lgl()\n",
    "# create the plot\n",
    "plt = igraph.plot(g, layout = lay, vertex_size = 15,bbox = (6000,6000))\n",
    "# and save it as, do to his weight, it is tricky to visualize inside the notebook\n",
    "plt.save('redplot2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[POSITIVE WEIGHT OF THE VERTECES IN THE MIDDLE.](https://www.flickr.com/photos/166751643@N04/31384086897/in/dateposted-public/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
